{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5dd42afa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dd42afa",
        "outputId": "b5b4081c-59f5-452d-c2a2-66952086ea7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-30 10:01:43--  http://www-public.tem-tsp.eu/~zhang_da/pub/dataset_tsmc2014.zip\n",
            "Resolving www-public.tem-tsp.eu (www-public.tem-tsp.eu)... 157.159.10.107, 2001:660:3203:100:1:0:80:107\n",
            "Connecting to www-public.tem-tsp.eu (www-public.tem-tsp.eu)|157.159.10.107|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25546284 (24M) [application/zip]\n",
            "Saving to: ‘dataset_tsmc2014.zip’\n",
            "\n",
            "dataset_tsmc2014.zi 100%[===================>]  24.36M  5.85MB/s    in 4.2s    \n",
            "\n",
            "2024-05-30 10:01:48 (5.85 MB/s) - ‘dataset_tsmc2014.zip’ saved [25546284/25546284]\n",
            "\n",
            "Archive:  dataset_tsmc2014.zip\n",
            "   creating: dataset_tsmc2014/\n",
            "  inflating: dataset_tsmc2014/dataset_TSMC2014_NYC.txt  \n",
            "  inflating: dataset_tsmc2014/dataset_TSMC2014_readme.txt  \n",
            "  inflating: dataset_tsmc2014/dataset_TSMC2014_TKY.txt  \n"
          ]
        }
      ],
      "source": [
        "#import pandas as pd\n",
        "!wget http://www-public.tem-tsp.eu/~zhang_da/pub/dataset_tsmc2014.zip\n",
        "!unzip dataset_tsmc2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c201ab83",
      "metadata": {
        "id": "c201ab83"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "columns = ['User ID',\n",
        " 'Venue ID',\n",
        " 'Venue category ID',\n",
        " 'Venue category name',\n",
        " 'Latitude',\n",
        " 'Longitude',\n",
        " 'Timezone',\n",
        " 'UTC time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a36d1da",
      "metadata": {
        "id": "1a36d1da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4207044b-d3bd-490c-abb1-5fce3117f08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        User ID                  Venue ID         Venue category ID  \\\n",
            "145653        7  42911d00f964a520f5231fe3  4bf58dd8d48988d129951735   \n",
            "125060      955  4bd879e909ecb713f3f0487c  4bf58dd8d48988d10f951735   \n",
            "185775      296  4b53367af964a5208e9227e3  4bf58dd8d48988d137941735   \n",
            "16154       225  4cbb7aa043ec6dcb1c32ab31  4bf58dd8d48988d171941735   \n",
            "227217      185  4d1b81e99aa5548136c5cee0  4bf58dd8d48988d130941735   \n",
            "...         ...                       ...                       ...   \n",
            "75997       704  4a6645fcf964a52051c81fe3  4bf58dd8d48988d1e4931735   \n",
            "59373        63  423e0e80f964a52048201fe3  4bf58dd8d48988d1d2941735   \n",
            "179928      734  4ecee40d02d5feaa1a85a14d  4bf58dd8d48988d1f6931735   \n",
            "92829       189  4c52fdd7048b1b8debbf4f31  4bf58dd8d48988d1cb941735   \n",
            "28801       906  4a85cc18f964a5204aff1fe3  4bf58dd8d48988d10c951735   \n",
            "\n",
            "         Venue category name   Latitude  Longitude  Timezone  \\\n",
            "145653         Train Station  40.750795 -73.993576      -240   \n",
            "125060  Drugstore / Pharmacy  40.698806 -74.231428      -240   \n",
            "185775               Theater  40.763733 -73.979493      -300   \n",
            "16154            Event Space  40.679242 -73.862806      -240   \n",
            "227217              Building  40.965166 -74.062895      -300   \n",
            "...                      ...        ...        ...       ...   \n",
            "75997          Bowling Alley  40.758296 -74.043498      -240   \n",
            "59373       Sushi Restaurant  40.712852 -73.957599      -240   \n",
            "179928        General Travel  40.943813 -74.016129      -300   \n",
            "92829             Food Truck  40.761122 -73.979509      -240   \n",
            "28801         Cosmetics Shop  40.742640 -73.993134      -240   \n",
            "\n",
            "                              UTC time  \n",
            "145653  Fri Aug 10 19:01:04 +0000 2012  \n",
            "125060  Sat Jul 07 19:00:56 +0000 2012  \n",
            "185775  Sat Dec 01 00:54:13 +0000 2012  \n",
            "16154   Sat Apr 14 19:07:30 +0000 2012  \n",
            "227217  Thu Feb 14 02:43:48 +0000 2013  \n",
            "...                                ...  \n",
            "75997   Sat May 19 04:00:35 +0000 2012  \n",
            "59373   Fri May 11 00:45:33 +0000 2012  \n",
            "179928  Tue Nov 20 20:09:28 +0000 2012  \n",
            "92829   Sun Jun 03 04:34:57 +0000 2012  \n",
            "28801   Sat Apr 21 15:13:27 +0000 2012  \n",
            "\n",
            "[10000 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "NUMCHECKIN = 10000\n",
        "\n",
        "df = pd.read_csv('dataset_tsmc2014/dataset_TSMC2014_NYC.txt', sep='\\t', encoding='latin-1', names=columns)\n",
        "df = df.sample(NUMCHECKIN)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ab15ac1",
      "metadata": {
        "id": "6ab15ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4fb124-82cd-4e41-94a9-f114a9762e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero di utenti con meno di 5 check-in: 236\n"
          ]
        }
      ],
      "source": [
        "\n",
        "checkin_counts = df['User ID'].value_counts() # Conta il numero di check-in per ogni utente\n",
        "users_with_sufficient_checkins = checkin_counts[checkin_counts >= 5].index   # Filtra gli utenti con meno di 5 check-in\n",
        "df = df[df['User ID'].isin(users_with_sufficient_checkins)]  # Mantieni solo le righe relative agli utenti con almeno 5 check-in\n",
        "\n",
        "print(f\"Numero di utenti con meno di 5 check-in: {len(checkin_counts[checkin_counts < 5])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "-ByntkrZYdH8",
      "metadata": {
        "id": "-ByntkrZYdH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c774e7-e593-424b-e0f8-dc60bb2326ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        User ID  Venue ID  Venue category ID   Latitude  Longitude  Timezone  \\\n",
            "145653        7       326                 76  40.750795 -73.993576      -240   \n",
            "125060      955      2867                 29  40.698806 -74.231428      -240   \n",
            "185775      296      2055                 94  40.763733 -73.979493      -300   \n",
            "16154       225      3778                143  40.679242 -73.862806      -240   \n",
            "227217      185      3995                 87  40.965166 -74.062895      -300   \n",
            "...         ...       ...                ...        ...        ...       ...   \n",
            "75997       704      1032                244  40.758296 -74.043498      -240   \n",
            "59373        63       302                223  40.712852 -73.957599      -240   \n",
            "179928      734      5104                270  40.943813 -74.016129      -300   \n",
            "92829       189      3447                217  40.761122 -73.979509      -240   \n",
            "28801       906      1211                 24  40.742640 -73.993134      -240   \n",
            "\n",
            "                              UTC time  \n",
            "145653  Fri Aug 10 19:01:04 +0000 2012  \n",
            "125060  Sat Jul 07 19:00:56 +0000 2012  \n",
            "185775  Sat Dec 01 00:54:13 +0000 2012  \n",
            "16154   Sat Apr 14 19:07:30 +0000 2012  \n",
            "227217  Thu Feb 14 02:43:48 +0000 2013  \n",
            "...                                ...  \n",
            "75997   Sat May 19 04:00:35 +0000 2012  \n",
            "59373   Fri May 11 00:45:33 +0000 2012  \n",
            "179928  Tue Nov 20 20:09:28 +0000 2012  \n",
            "92829   Sun Jun 03 04:34:57 +0000 2012  \n",
            "28801   Sat Apr 21 15:13:27 +0000 2012  \n",
            "\n",
            "[9285 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Venue ID'] = le.fit_transform(df['Venue ID'])\n",
        "df['Venue category ID'] = le.fit_transform(df['Venue category ID'])\n",
        "df.drop(['Venue category name'],axis=1,inplace=True)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "184078b5",
      "metadata": {
        "id": "184078b5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def distance_between_coordinates(lat1, lon1, lat2, lon2):\n",
        "    radius = 6371  # Radius of the Earth in km\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    lat1 = math.radians(lat1)\n",
        "    lon1 = math.radians(lon1)\n",
        "    lat2 = math.radians(lat2)\n",
        "    lon2 = math.radians(lon2)\n",
        "\n",
        "    # Calculate the differences in latitude and longitude<\n",
        "    d_lat = lat2 - lat1\n",
        "    d_lon = lon2 - lon1\n",
        "\n",
        "    # Calculate the distance using the Haversine formula\n",
        "    a = math.sin(d_lat/2) * math.sin(d_lat/2) + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon/2) * math.sin(d_lon/2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    distance = radius * c\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9a7233d1",
      "metadata": {
        "id": "9a7233d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85db0576-deb2-4f73-e54f-db088ad28a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8819941850086326\n"
          ]
        }
      ],
      "source": [
        "print(distance_between_coordinates(40.719810,-74.002581,40.735981, -74.029309))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "IyhWcu8Hb2Hj",
      "metadata": {
        "id": "IyhWcu8Hb2Hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563212f4-9dd8-4dd5-f01d-cc4226029c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f85843e15d2c>:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['localTime'] = pd.to_datetime(df['UTC time']) + pd.to_timedelta(df['Timezone'], unit = 'm')\n"
          ]
        }
      ],
      "source": [
        "#build an alternative temporal graph\n",
        "#two pois are connected if users go on average to them in the seme timeframe\n",
        "\n",
        "df['localTime'] = pd.to_datetime(df['UTC time']) + pd.to_timedelta(df['Timezone'], unit = 'm')\n",
        "\n",
        "df['localDayofWeek'] = df.localTime.dt.dayofweek\n",
        "df['localHour'] = df.localTime.dt.hour\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f614c60a",
      "metadata": {
        "id": "f614c60a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca68fcb-61dd-4e1f-e019-67f4e8b0a0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Venue ID   Latitude  Longitude  Venue category ID\n",
            "145653       326  40.750795 -73.993576                 76\n",
            "125060      2867  40.698806 -74.231428                 29\n",
            "185775      2055  40.763733 -73.979493                 94\n",
            "16154       3778  40.679242 -73.862806                143\n",
            "227217      3995  40.965166 -74.062895                 87\n",
            "...          ...        ...        ...                ...\n",
            "200810      4862  40.757305 -73.968640                 41\n",
            "75997       1032  40.758296 -74.043498                244\n",
            "59373        302  40.712852 -73.957599                223\n",
            "92829       3447  40.761122 -73.979509                217\n",
            "28801       1211  40.742640 -73.993134                 24\n",
            "\n",
            "[5911 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "venuesdf = df[['Venue ID','Latitude','Longitude','Venue category ID']].drop_duplicates(subset=['Venue ID'])\n",
        "numvenues = len(venuesdf)\n",
        "print(venuesdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wOEGNi0V-mhi",
      "metadata": {
        "id": "wOEGNi0V-mhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ddd2c4-9f81-4a57-b2f5-d2dbc0b56825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m894.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch --index-url https://download.pytorch.org/whl/${CUDA}\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse \\\n",
        "  torch-scatter \\\n",
        "  torch-cluster \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-2.3.0+cu121.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peLIawpuCpsl",
      "metadata": {
        "id": "peLIawpuCpsl"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "\n",
        "i = 0\n",
        "nodeIds = {}\n",
        "node_attr = []\n",
        "\n",
        "for el in venuesdict:\n",
        "  #print(el['Venue ID'])\n",
        "  nodeIds[el['Venue ID']] = i\n",
        "  node_attr.append([float(el['Venue category ID'])])\n",
        "  i+=1\n",
        "\n",
        "print(len(node_attr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gi8Q9FEIo9Ef",
      "metadata": {
        "id": "Gi8Q9FEIo9Ef"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "sequences = {}\n",
        "count = {}\n",
        "\n",
        "MINVISIT = 5\n",
        "\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "\n",
        "  if row['User ID'] not in sequences:\n",
        "    sequences[row['User ID']] = [[row['Venue ID'],row['localHour'],row['localDayofWeek']]]\n",
        "\n",
        "\n",
        "  else:\n",
        "    sequences[row['User ID']].append([row['Venue ID'],row['localHour'],row['localDayofWeek']])\n",
        "\n",
        "\n",
        "  if row['Venue ID'] not in count:\n",
        "    count[row['Venue ID']] = 1\n",
        "  else:\n",
        "    count[row['Venue ID']] += 1\n",
        "\n",
        "print(sequences)\n",
        "seq = []\n",
        "next = []\n",
        "to_remove = []\n",
        "\n",
        "for el in sequences:\n",
        "  entry = [el]\n",
        "  for i in range(len(sequences[el][len(sequences[el])-1])):\n",
        "    el2 = sequences[el][len(sequences[el])-1][i]\n",
        "    #print(el2,i)\n",
        "    if i == 0:\n",
        "      el2 = nodeIds[el2]\n",
        "    #print(el2,i)\n",
        "    entry.append(el2)\n",
        "  for i in range(len(sequences[el])-1):\n",
        "    entry.append(nodeIds[sequences[el][i][0]])\n",
        "  seq.append(torch.tensor(entry))\n",
        "\n",
        "  target = torch.zeros(numvenues)\n",
        "  target[sequences[el][len(sequences[el])-1][0]] = 1\n",
        "  next.append(target)\n",
        "  #print(count[sequences[el][len(sequences[el])-1][0]])\n",
        "  if count[sequences[el][len(sequences[el])-1][0]] < MINVISIT:\n",
        "\n",
        "    to_remove.append(sequences[el][len(sequences[el])-1][0])\n",
        "    #seq.append(torch.tensor(entry))\n",
        "    #next.append(target)\n",
        "\n",
        "seq=torch.nn.utils.rnn.pad_sequence(seq,batch_first=True)\n",
        "seq_dim = len(seq[0])-4\n",
        "print(seq_dim)\n",
        "#seq=torch.nn.utils.rnn.pad_sequence(seq)\n",
        "\n",
        "print(len(to_remove))\n",
        "\n",
        "#to_remove_new = []\n",
        "\n",
        "#for el in to_remove:\n",
        "\n",
        "#  el = nodeIds[el]\n",
        "#  to_remove_new[]\n",
        "\n",
        "print(seq)\n",
        "print(len(next))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WQcL9QUDo7kz",
      "metadata": {
        "id": "WQcL9QUDo7kz"
      },
      "outputs": [],
      "source": [
        "df = df[~df['Venue ID'].isin(to_remove)]\n",
        "venuesdf = venuesdf[~venuesdf['Venue ID'].isin(to_remove)]\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "print(df)\n",
        "print(venuesdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LoL_BQyKFRJL",
      "metadata": {
        "id": "LoL_BQyKFRJL"
      },
      "outputs": [],
      "source": [
        "#fast spatial graph construction\n",
        "\n",
        "import networkx as nx\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "#print(venuesdict)\n",
        "\n",
        "#for el in venuesdict:\n",
        "#  print((el,venuesdict[el]))\n",
        "'''\n",
        "G = nx.Graph()\n",
        "\n",
        "distances = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(venuesdict) - 1)):\n",
        "    if not G.has_node(venuesdict[i]['Venue ID']):\n",
        "        G.add_node(venuesdict[i]['Venue ID'], category=venuesdict[i]['Venue category ID'])\n",
        "\n",
        "    for j in range(i + 1, len(venuesdict)):\n",
        "        if not G.has_node(venuesdict[j]['Venue ID']):\n",
        "            G.add_node(venuesdict[j]['Venue ID'], category=venuesdict[i]['Venue category ID'])\n",
        "\n",
        "        if distance_between_coordinates(venuesdict[i]['Latitude'], venuesdict[i]['Longitude'],\n",
        "                                        venuesdict[j]['Latitude'], venuesdict[j]['Longitude']) < 1:\n",
        "            G.add_edge(venuesdict[i]['Venue ID'], venuesdict[j]['Venue ID'])\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "for i in tqdm(range(len(venuesdict) - 1)):\n",
        "\n",
        "   #if venuesdict[i]['Venue ID'] not in node_attr:\n",
        "   #  node_attr.append(venuesdict[i]['Venue ID'])\n",
        "\n",
        "   for j in range(i + 1, len(venuesdict)):\n",
        "    #  if venuesdict[j]['Venue ID'] not in node_attr:\n",
        "    #     node_attr.append(venuesdict[j]['Venue ID'])\n",
        "\n",
        "\n",
        "      if distance_between_coordinates(venuesdict[i]['Latitude'], venuesdict[i]['Longitude'],\n",
        "                                    venuesdict[j]['Latitude'], venuesdict[j]['Longitude']) < 1:\n",
        "        edge_index.append([nodeIds[venuesdict[i]['Venue ID']], nodeIds[venuesdict[j]['Venue ID']]])\n",
        "        edges_attr.append([1])\n",
        "\n",
        "spatG = Data(x=torch.tensor(node_attr), edge_index=torch.LongTensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(spatG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Blc1bRFSR2cA",
      "metadata": {
        "id": "Blc1bRFSR2cA"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "print(spatG)\n",
        "\n",
        "#spatpG = G\n",
        "\n",
        "pickle.dump(spatG,open('spatG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('spatG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mxp8l8v7SyTw",
      "metadata": {
        "id": "Mxp8l8v7SyTw"
      },
      "outputs": [],
      "source": [
        "#nx.draw(G)\n",
        "print(spatG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ab55e3",
      "metadata": {
        "id": "e9ab55e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#sequantial relation graph\n",
        "#build a graph that captures the sequence of actions made by the users\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "usrloc = {}\n",
        "\n",
        "#seqG = nx.Graph()\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "\n",
        "  #if not seqG.has_node(row['Venue ID']):\n",
        "  #  seqG.add_node(row['Venue ID'], category=row['Venue category ID'])\n",
        "\n",
        "  if row['User ID'] not in usrloc:\n",
        "    usrloc[row['User ID']] = row['Venue ID']\n",
        "\n",
        "  else:\n",
        "    #seqG.add_edge(usrloc[row['User ID']],row['Venue ID'])\n",
        "    edge_index.append([nodeIds[usrloc[row['User ID']]], nodeIds[row['Venue ID']]])\n",
        "    edges_attr.append(1)\n",
        "    usrloc[row['User ID']] = row['Venue ID']\n",
        "\n",
        "\n",
        "seqG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(seqG)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i_gOIQACbN1E",
      "metadata": {
        "id": "i_gOIQACbN1E"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "print(seqG)\n",
        "\n",
        "pickle.dump(seqG,open('tempG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('tempG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y7d_3DktgSmi",
      "metadata": {
        "id": "y7d_3DktgSmi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "groupeddf = df.groupby(['Venue ID','Venue category ID'],as_index=False)\n",
        "avghour = groupeddf['localHour'].mean()\n",
        "avgday = groupeddf['localDayofWeek'].mean()\n",
        "\n",
        "print(avghour)\n",
        "\n",
        "timedf = pd.concat([avghour,avgday['localDayofWeek']],axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(timedf)\n",
        "\n",
        "hours = {}\n",
        "week = {}\n",
        "\n",
        "#week['workdays'] = []\n",
        "#week['weekend'] = []\n",
        "\n",
        "\n",
        "for _, row in timedf.iterrows():\n",
        "\n",
        "  hour = math.floor(row['localHour'])\n",
        "  day = math.floor(row['localDayofWeek'])\n",
        "\n",
        "  if hour not in hours:\n",
        "    #hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "    hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "\n",
        "  else:\n",
        "    hours[hour].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "  #if 1 <= day <=4:\n",
        "  #  week['workdays'].append([row['Venue ID'],row['Venue category ID']])\n",
        "  #else:\n",
        "  #  week['weekend'].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "  if day not in week:\n",
        "    #hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "    week[day] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "\n",
        "  else:\n",
        "    week[day].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUL95Z8_CV2E",
      "metadata": {
        "id": "sUL95Z8_CV2E"
      },
      "outputs": [],
      "source": [
        "#dayG = nx.Graph()\n",
        "#hourG = nx.Graph()\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "for el in hours:\n",
        "   for u in hours[el]:\n",
        "\n",
        "    #if not hourG.has_node(u[0]):\n",
        "    #  hourG.add_node(u[0], category=u[1])\n",
        "\n",
        "    for v in hours[el]:\n",
        "\n",
        "      if u != v:\n",
        "\n",
        "        #if not hourG.has_node(u[0]):\n",
        "        #  hourG.add_node(v[0], category=v[1])\n",
        "\n",
        "        #hourG.add_edge(u[0],v[0])\n",
        "        edge_index.append([nodeIds[u[0]], nodeIds[v[0]]])\n",
        "\n",
        "hourG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(hourG)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef64bc7",
      "metadata": {
        "id": "5ef64bc7"
      },
      "outputs": [],
      "source": [
        "pickle.dump(hourG,open('hourG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('tempG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DVzk8w57C8b6",
      "metadata": {
        "id": "DVzk8w57C8b6"
      },
      "outputs": [],
      "source": [
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "\n",
        "for el in week:\n",
        "   for u in week[el]:\n",
        "\n",
        "    #if not dayG.has_node(u[0]):\n",
        "    #  dayG.add_node(u[0], category=u[1])\n",
        "\n",
        "    for v in week[el]:\n",
        "\n",
        "      if u != v:\n",
        "\n",
        "        #if not dayG.has_node(u[0]):\n",
        "        #  dayG.add_node(v[0], category=v[1])\n",
        "\n",
        "        #dayG.add_edge(u[0],v[0])\n",
        "        edge_index.append([nodeIds[u[0]], nodeIds[v[0]]])\n",
        "\n",
        "dayG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(dayG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3ffb10",
      "metadata": {
        "id": "bc3ffb10"
      },
      "outputs": [],
      "source": [
        "pickle.dump(dayG,open('dayG.pickle','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XlLW_es1-Z87",
      "metadata": {
        "id": "XlLW_es1-Z87"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# Lightning Module for Node2Vec model\n",
        "class Node2VecModel(pl.LightningModule):\n",
        "    def __init__(self, data, embedding_dim=32, walk_length=20, context_size=10, walks_per_node=10, num_negative_samples=1, p=1.0, q=1.0, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.model = Node2Vec(\n",
        "            data.edge_index,\n",
        "            embedding_dim=embedding_dim,\n",
        "            walk_length=walk_length,\n",
        "            context_size=context_size,\n",
        "            walks_per_node=walks_per_node,\n",
        "            num_negative_samples=num_negative_samples,\n",
        "            p=p,\n",
        "            q=q,\n",
        "            sparse=True\n",
        "        )\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self):\n",
        "        return self.model()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pos_rw, neg_rw = batch\n",
        "        loss = self.model.loss(pos_rw.to(self.device), neg_rw.to(self.device))\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SparseAdam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        z = self.model()\n",
        "        acc = self.model.test(\n",
        "            train_z=z[self.data.train_mask],\n",
        "            train_y=self.data.y[self.data.train_mask],\n",
        "            test_z=z[self.data.test_mask],\n",
        "            test_y=self.data.y[self.data.test_mask],\n",
        "            max_iter=150\n",
        "        )\n",
        "        self.log('test_acc', acc)\n",
        "        return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = 4\n",
        "graph_embed = []\n",
        "\n",
        "for el in [spatG,hourG,seqG,dayG]:\n",
        "\n",
        "  model = Node2VecModel(data=el)\n",
        "\n",
        "  loader = model.model.loader(batch_size=128, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=100,\n",
        "      callbacks=[\n",
        "          ModelCheckpoint(monitor='train_loss'),\n",
        "          EarlyStopping(monitor='train_loss', patience=10)\n",
        "      ]\n",
        "  )\n",
        "  trainer.fit(model, loader)\n",
        "  #trainer.test(model, loader)\n",
        "\n",
        "  # Plotting points\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  z = model().detach()\n",
        "  graph_embed.append(z)\n",
        "\n",
        "  #z = TSNE(n_components=2).fit_transform(z)\n",
        "  print(z.shape)\n"
      ],
      "metadata": {
        "id": "-pCdEuFKhgcr"
      },
      "id": "-pCdEuFKhgcr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QxAZBv8Y-udN",
      "metadata": {
        "id": "QxAZBv8Y-udN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "#train_x,test_x,train_y,test_y = train_test_split(seq,torch.stack(next),test_size=0.2,stratify=torch.stack(next))\n",
        "train_x,test_x,train_y,test_y = train_test_split(seq,torch.stack(next),test_size=0.2)\n",
        "#train_x,val_x,train_y,val_y = train_test_split(train_x,train_y,test_size=0.2,stratify=train_y)\n",
        "train_x,val_x,train_y,val_y = train_test_split(train_x,train_y,test_size=0.2)\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(train_x,train_y)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=batchsize,shuffle=True)\n",
        "print(len(trainloader))\n",
        "\n",
        "valset = torch.utils.data.TensorDataset(val_x,val_y)\n",
        "valloader = torch.utils.data.DataLoader(valset,batch_size=batchsize,shuffle=True)\n",
        "print(len(valloader))\n",
        "\n",
        "testset = torch.utils.data.TensorDataset(test_x,test_y)\n",
        "testloader = torch.utils.data.DataLoader(testset,batch_size=batchsize,shuffle=True)\n",
        "print(len(testloader))\n",
        "\n",
        "datasets = [trainset,valset,testset]\n",
        "\n",
        "pickle.dump(datasets,open('datasets.pickle','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class GCN(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class LightningModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim, location_dim, sequence_dim, graph_embed):\n",
        "        super(LightningModel, self).__init__()\n",
        "        self.graph_embed = graph_embed\n",
        "\n",
        "        # User Embedding Layer\n",
        "        self.user_embedding = torch.nn.Embedding(num_embeddings=input_dim, embedding_dim=32)\n",
        "\n",
        "        # Last location Embedding Layer\n",
        "        self.lastLoc_embedding = torch.nn.Embedding(num_embeddings=input_dim, embedding_dim=32)\n",
        "\n",
        "        # Time and day Input linear\n",
        "        self.timeday_embedding = torch.nn.Linear(2, 32)\n",
        "\n",
        "        # Sequence Embedding Layer\n",
        "        self.self_attn = torch.nn.MultiheadAttention(embed_dim=sequence_dim, num_heads=1)\n",
        "\n",
        "        # Expand sequence representation\n",
        "        self.expand_sequence = torch.nn.Linear(sequence_dim, 32)\n",
        "\n",
        "        # Multihead Attention Layer\n",
        "        self.multihead_attn = torch.nn.MultiheadAttention(embed_dim=32, num_heads=2)\n",
        "\n",
        "        # Classifier Layers\n",
        "        self.linear1 = torch.nn.Linear(int((4*32*(self.graph_embed[0].shape[0]+batchsize))/batchsize), 128)\n",
        "        self.linear2 = torch.nn.Linear(128, 64)\n",
        "        self.linear3 = torch.nn.Linear(64, numvenues)\n",
        "\n",
        "    def forward(self, user, lastloc, lasthour, lastday, sequence):\n",
        "        user_embed = self.user_embedding(user.int())\n",
        "        lastloc_embed = self.lastLoc_embedding(lastloc.int())\n",
        "        timeday_embed = self.timeday_embedding(torch.stack([lasthour.float(), lastday.float()], dim=1))\n",
        "        graph_embed = torch.cat(self.graph_embed)\n",
        "\n",
        "        # Sequence Embedding\n",
        "        sequence_embed, _ = self.self_attn(sequence.float(), sequence.float(), sequence.float())\n",
        "        sequence_embed = self.expand_sequence(sequence_embed)\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        concat_embed = torch.cat([user_embed, lastloc_embed, graph_embed.to(self.device), timeday_embed, sequence_embed], dim=0)\n",
        "\n",
        "        # Multihead Attention\n",
        "        attn_output, _ = self.multihead_attn(concat_embed, concat_embed, concat_embed)\n",
        "\n",
        "        # Classifier\n",
        "        out = F.relu(self.linear1(attn_output.view(batchsize, -1)))\n",
        "        out = F.relu(self.linear2(out))\n",
        "        out = self.linear3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        x, y = batch\n",
        "        pad_dim = batchsize - x.shape[0]\n",
        "        if pad_dim != 0:\n",
        "            x = torch.cat([x, torch.zeros(pad_dim, x.shape[1]).to(self.device)])\n",
        "            y = torch.cat([y, torch.zeros(pad_dim, y.shape[1]).to(self.device)])\n",
        "        y_pred = self(x[:, 0], x[:, 1], x[:, 2], x[:, 3], x[:, 4:])\n",
        "        loss = F.cross_entropy(torch.sigmoid(y_pred), y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Esempio di addestramento del modello\n",
        "input_dim = 10000\n",
        "location_dim = 32\n",
        "sequence_dim = seq_dim\n",
        "model = LightningModel(input_dim, location_dim, sequence_dim,graph_embed)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=1000, log_every_n_steps=1)\n",
        "trainer.fit(model, trainloader)\n"
      ],
      "metadata": {
        "id": "vEP4uZCJmkNt"
      },
      "id": "vEP4uZCJmkNt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}