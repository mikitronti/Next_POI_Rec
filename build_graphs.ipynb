{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd42afa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dd42afa",
        "outputId": "6769ffba-bf65-48ff-cfcb-418009ef228a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-29 13:44:41--  http://www-public.tem-tsp.eu/~zhang_da/pub/dataset_tsmc2014.zip\n",
            "Resolving www-public.tem-tsp.eu (www-public.tem-tsp.eu)... 157.159.10.107, 2001:660:3203:100:1:0:80:107\n",
            "Connecting to www-public.tem-tsp.eu (www-public.tem-tsp.eu)|157.159.10.107|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25546284 (24M) [application/zip]\n",
            "Saving to: ‘dataset_tsmc2014.zip.8’\n",
            "\n",
            "dataset_tsmc2014.zi 100%[===================>]  24.36M  7.15MB/s    in 4.6s    \n",
            "\n",
            "2024-05-29 13:44:46 (5.31 MB/s) - ‘dataset_tsmc2014.zip.8’ saved [25546284/25546284]\n",
            "\n",
            "Archive:  dataset_tsmc2014.zip\n",
            "replace dataset_tsmc2014/dataset_TSMC2014_NYC.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#import pandas as pd\n",
        "!wget http://www-public.tem-tsp.eu/~zhang_da/pub/dataset_tsmc2014.zip\n",
        "!unzip dataset_tsmc2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c201ab83",
      "metadata": {
        "id": "c201ab83"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "columns = ['User ID',\n",
        " 'Venue ID',\n",
        " 'Venue category ID',\n",
        " 'Venue category name',\n",
        " 'Latitude',\n",
        " 'Longitude',\n",
        " 'Timezone',\n",
        " 'UTC time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a36d1da",
      "metadata": {
        "id": "1a36d1da"
      },
      "outputs": [],
      "source": [
        "NUMCHECKIN = 7000\n",
        "\n",
        "df = pd.read_csv('dataset_tsmc2014/dataset_TSMC2014_NYC.txt', sep='\\t', encoding='latin-1', names=columns)\n",
        "df = df.sample(NUMCHECKIN)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab15ac1",
      "metadata": {
        "id": "6ab15ac1"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkin_counts = df['User ID'].value_counts() # Conta il numero di check-in per ogni utente\n",
        "users_with_sufficient_checkins = checkin_counts[checkin_counts >= 5].index   # Filtra gli utenti con meno di 5 check-in\n",
        "df = df[df['User ID'].isin(users_with_sufficient_checkins)]  # Mantieni solo le righe relative agli utenti con almeno 5 check-in\n",
        "\n",
        "print(f\"Numero di utenti con meno di 5 check-in: {len(checkin_counts[checkin_counts < 5])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ByntkrZYdH8",
      "metadata": {
        "id": "-ByntkrZYdH8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Venue ID'] = le.fit_transform(df['Venue ID'])\n",
        "df['Venue category ID'] = le.fit_transform(df['Venue category ID'])\n",
        "df.drop(['Venue category name'],axis=1,inplace=True)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184078b5",
      "metadata": {
        "id": "184078b5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def distance_between_coordinates(lat1, lon1, lat2, lon2):\n",
        "    radius = 6371  # Radius of the Earth in km\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    lat1 = math.radians(lat1)\n",
        "    lon1 = math.radians(lon1)\n",
        "    lat2 = math.radians(lat2)\n",
        "    lon2 = math.radians(lon2)\n",
        "\n",
        "    # Calculate the differences in latitude and longitude<\n",
        "    d_lat = lat2 - lat1\n",
        "    d_lon = lon2 - lon1\n",
        "\n",
        "    # Calculate the distance using the Haversine formula\n",
        "    a = math.sin(d_lat/2) * math.sin(d_lat/2) + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon/2) * math.sin(d_lon/2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    distance = radius * c\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7233d1",
      "metadata": {
        "id": "9a7233d1"
      },
      "outputs": [],
      "source": [
        "print(distance_between_coordinates(40.719810,-74.002581,40.735981, -74.029309))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IyhWcu8Hb2Hj",
      "metadata": {
        "id": "IyhWcu8Hb2Hj"
      },
      "outputs": [],
      "source": [
        "#build an alternative temporal graph\n",
        "#two pois are connected if users go on average to them in the seme timeframe\n",
        "\n",
        "df['localTime'] = pd.to_datetime(df['UTC time']) + pd.to_timedelta(df['Timezone'], unit = 'm')\n",
        "\n",
        "df['localDayofWeek'] = df.localTime.dt.dayofweek\n",
        "df['localHour'] = df.localTime.dt.hour\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f614c60a",
      "metadata": {
        "id": "f614c60a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "venuesdf = df[['Venue ID','Latitude','Longitude','Venue category ID']].drop_duplicates(subset=['Venue ID'])\n",
        "numvenues = len(venuesdf)\n",
        "print(venuesdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wOEGNi0V-mhi",
      "metadata": {
        "id": "wOEGNi0V-mhi"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch --index-url https://download.pytorch.org/whl/${CUDA}\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse \\\n",
        "  torch-scatter \\\n",
        "  torch-cluster \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-2.3.0+cu121.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peLIawpuCpsl",
      "metadata": {
        "id": "peLIawpuCpsl"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "\n",
        "i = 0\n",
        "nodeIds = {}\n",
        "node_attr = []\n",
        "\n",
        "for el in venuesdict:\n",
        "  #print(el['Venue ID'])\n",
        "  nodeIds[el['Venue ID']] = i\n",
        "  node_attr.append([float(el['Venue category ID'])])\n",
        "  i+=1\n",
        "\n",
        "print(len(node_attr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gi8Q9FEIo9Ef",
      "metadata": {
        "id": "Gi8Q9FEIo9Ef"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "sequences = {}\n",
        "count = {}\n",
        "\n",
        "MINVISIT = 5\n",
        "\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "\n",
        "  if row['User ID'] not in sequences:\n",
        "    sequences[row['User ID']] = [[row['Venue ID'],row['localHour'],row['localDayofWeek']]]\n",
        "\n",
        "\n",
        "  else:\n",
        "    sequences[row['User ID']].append([row['Venue ID'],row['localHour'],row['localDayofWeek']])\n",
        "\n",
        "\n",
        "  if row['Venue ID'] not in count:\n",
        "    count[row['Venue ID']] = 1\n",
        "  else:\n",
        "    count[row['Venue ID']] += 1\n",
        "\n",
        "print(sequences)\n",
        "seq = []\n",
        "next = []\n",
        "to_remove = []\n",
        "\n",
        "for el in sequences:\n",
        "  entry = [el]\n",
        "  for i in range(len(sequences[el][len(sequences[el])-1])):\n",
        "    el2 = sequences[el][len(sequences[el])-1][i]\n",
        "    #print(el2,i)\n",
        "    if i == 0:\n",
        "      el2 = nodeIds[el2]\n",
        "    #print(el2,i)\n",
        "    entry.append(el2)\n",
        "  for i in range(len(sequences[el])-1):\n",
        "    entry.append(nodeIds[sequences[el][i][0]])\n",
        "  seq.append(torch.tensor(entry))\n",
        "\n",
        "  target = torch.zeros(numvenues)\n",
        "  target[sequences[el][len(sequences[el])-1][0]] = 1\n",
        "  next.append(target)\n",
        "  #print(count[sequences[el][len(sequences[el])-1][0]])\n",
        "  if count[sequences[el][len(sequences[el])-1][0]] < MINVISIT:\n",
        "\n",
        "    to_remove.append(sequences[el][len(sequences[el])-1][0])\n",
        "    #seq.append(torch.tensor(entry))\n",
        "    #next.append(target)\n",
        "\n",
        "seq=torch.nn.utils.rnn.pad_sequence(seq,batch_first=True)\n",
        "seq_dim = len(seq[0])-4\n",
        "print(seq_dim)\n",
        "#seq=torch.nn.utils.rnn.pad_sequence(seq)\n",
        "\n",
        "print(len(to_remove))\n",
        "\n",
        "#to_remove_new = []\n",
        "\n",
        "#for el in to_remove:\n",
        "\n",
        "#  el = nodeIds[el]\n",
        "#  to_remove_new[]\n",
        "\n",
        "print(seq)\n",
        "print(len(next))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WQcL9QUDo7kz",
      "metadata": {
        "id": "WQcL9QUDo7kz"
      },
      "outputs": [],
      "source": [
        "df = df[~df['Venue ID'].isin(to_remove)]\n",
        "venuesdf = venuesdf[~venuesdf['Venue ID'].isin(to_remove)]\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "print(df)\n",
        "print(venuesdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LoL_BQyKFRJL",
      "metadata": {
        "id": "LoL_BQyKFRJL"
      },
      "outputs": [],
      "source": [
        "#fast spatial graph construction\n",
        "\n",
        "import networkx as nx\n",
        "venuesdict = venuesdf.to_dict('records')\n",
        "#print(venuesdict)\n",
        "\n",
        "#for el in venuesdict:\n",
        "#  print((el,venuesdict[el]))\n",
        "'''\n",
        "G = nx.Graph()\n",
        "\n",
        "distances = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(venuesdict) - 1)):\n",
        "    if not G.has_node(venuesdict[i]['Venue ID']):\n",
        "        G.add_node(venuesdict[i]['Venue ID'], category=venuesdict[i]['Venue category ID'])\n",
        "\n",
        "    for j in range(i + 1, len(venuesdict)):\n",
        "        if not G.has_node(venuesdict[j]['Venue ID']):\n",
        "            G.add_node(venuesdict[j]['Venue ID'], category=venuesdict[i]['Venue category ID'])\n",
        "\n",
        "        if distance_between_coordinates(venuesdict[i]['Latitude'], venuesdict[i]['Longitude'],\n",
        "                                        venuesdict[j]['Latitude'], venuesdict[j]['Longitude']) < 1:\n",
        "            G.add_edge(venuesdict[i]['Venue ID'], venuesdict[j]['Venue ID'])\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "for i in tqdm(range(len(venuesdict) - 1)):\n",
        "\n",
        "   #if venuesdict[i]['Venue ID'] not in node_attr:\n",
        "   #  node_attr.append(venuesdict[i]['Venue ID'])\n",
        "\n",
        "   for j in range(i + 1, len(venuesdict)):\n",
        "    #  if venuesdict[j]['Venue ID'] not in node_attr:\n",
        "    #     node_attr.append(venuesdict[j]['Venue ID'])\n",
        "\n",
        "\n",
        "      if distance_between_coordinates(venuesdict[i]['Latitude'], venuesdict[i]['Longitude'],\n",
        "                                    venuesdict[j]['Latitude'], venuesdict[j]['Longitude']) < 1:\n",
        "        edge_index.append([nodeIds[venuesdict[i]['Venue ID']], nodeIds[venuesdict[j]['Venue ID']]])\n",
        "        edges_attr.append([1])\n",
        "\n",
        "spatG = Data(x=torch.tensor(node_attr), edge_index=torch.LongTensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(spatG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Blc1bRFSR2cA",
      "metadata": {
        "id": "Blc1bRFSR2cA"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "print(spatG)\n",
        "\n",
        "#spatpG = G\n",
        "\n",
        "pickle.dump(spatG,open('spatG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('spatG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mxp8l8v7SyTw",
      "metadata": {
        "id": "Mxp8l8v7SyTw"
      },
      "outputs": [],
      "source": [
        "#nx.draw(G)\n",
        "print(spatG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ab55e3",
      "metadata": {
        "id": "e9ab55e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#sequantial relation graph\n",
        "#build a graph that captures the sequence of actions made by the users\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "usrloc = {}\n",
        "\n",
        "#seqG = nx.Graph()\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "\n",
        "  #if not seqG.has_node(row['Venue ID']):\n",
        "  #  seqG.add_node(row['Venue ID'], category=row['Venue category ID'])\n",
        "\n",
        "  if row['User ID'] not in usrloc:\n",
        "    usrloc[row['User ID']] = row['Venue ID']\n",
        "\n",
        "  else:\n",
        "    #seqG.add_edge(usrloc[row['User ID']],row['Venue ID'])\n",
        "    edge_index.append([nodeIds[usrloc[row['User ID']]], nodeIds[row['Venue ID']]])\n",
        "    edges_attr.append(1)\n",
        "    usrloc[row['User ID']] = row['Venue ID']\n",
        "\n",
        "\n",
        "seqG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(seqG)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i_gOIQACbN1E",
      "metadata": {
        "id": "i_gOIQACbN1E"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "print(seqG)\n",
        "\n",
        "pickle.dump(seqG,open('tempG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('tempG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y7d_3DktgSmi",
      "metadata": {
        "id": "y7d_3DktgSmi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "groupeddf = df.groupby(['Venue ID','Venue category ID'],as_index=False)\n",
        "avghour = groupeddf['localHour'].mean()\n",
        "avgday = groupeddf['localDayofWeek'].mean()\n",
        "\n",
        "print(avghour)\n",
        "\n",
        "timedf = pd.concat([avghour,avgday['localDayofWeek']],axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(timedf)\n",
        "\n",
        "hours = {}\n",
        "week = {}\n",
        "\n",
        "#week['workdays'] = []\n",
        "#week['weekend'] = []\n",
        "\n",
        "\n",
        "for _, row in timedf.iterrows():\n",
        "\n",
        "  hour = math.floor(row['localHour'])\n",
        "  day = math.floor(row['localDayofWeek'])\n",
        "\n",
        "  if hour not in hours:\n",
        "    #hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "    hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "\n",
        "  else:\n",
        "    hours[hour].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "  #if 1 <= day <=4:\n",
        "  #  week['workdays'].append([row['Venue ID'],row['Venue category ID']])\n",
        "  #else:\n",
        "  #  week['weekend'].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "  if day not in week:\n",
        "    #hours[hour] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "    week[day] = [[row['Venue ID'],row['Venue category ID']]]\n",
        "\n",
        "  else:\n",
        "    week[day].append([row['Venue ID'],row['Venue category ID']])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUL95Z8_CV2E",
      "metadata": {
        "id": "sUL95Z8_CV2E"
      },
      "outputs": [],
      "source": [
        "#dayG = nx.Graph()\n",
        "#hourG = nx.Graph()\n",
        "\n",
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "for el in hours:\n",
        "   for u in hours[el]:\n",
        "\n",
        "    #if not hourG.has_node(u[0]):\n",
        "    #  hourG.add_node(u[0], category=u[1])\n",
        "\n",
        "    for v in hours[el]:\n",
        "\n",
        "      if u != v:\n",
        "\n",
        "        #if not hourG.has_node(u[0]):\n",
        "        #  hourG.add_node(v[0], category=v[1])\n",
        "\n",
        "        #hourG.add_edge(u[0],v[0])\n",
        "        edge_index.append([nodeIds[u[0]], nodeIds[v[0]]])\n",
        "\n",
        "hourG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(hourG)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef64bc7",
      "metadata": {
        "id": "5ef64bc7"
      },
      "outputs": [],
      "source": [
        "pickle.dump(hourG,open('hourG.pickle','wb'))\n",
        "\n",
        "#data = pickle.load(open('tempG.pickle','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DVzk8w57C8b6",
      "metadata": {
        "id": "DVzk8w57C8b6"
      },
      "outputs": [],
      "source": [
        "edge_index = []\n",
        "edges_attr = []\n",
        "\n",
        "\n",
        "for el in week:\n",
        "   for u in week[el]:\n",
        "\n",
        "    #if not dayG.has_node(u[0]):\n",
        "    #  dayG.add_node(u[0], category=u[1])\n",
        "\n",
        "    for v in week[el]:\n",
        "\n",
        "      if u != v:\n",
        "\n",
        "        #if not dayG.has_node(u[0]):\n",
        "        #  dayG.add_node(v[0], category=v[1])\n",
        "\n",
        "        #dayG.add_edge(u[0],v[0])\n",
        "        edge_index.append([nodeIds[u[0]], nodeIds[v[0]]])\n",
        "\n",
        "dayG = Data(x=torch.tensor(node_attr), edge_index=torch.tensor(edge_index).t().contiguous(), edge_attr=torch.tensor(edges_attr))\n",
        "print(dayG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3ffb10",
      "metadata": {
        "id": "bc3ffb10"
      },
      "outputs": [],
      "source": [
        "pickle.dump(dayG,open('dayG.pickle','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3RyeVXUsEF_D",
      "metadata": {
        "id": "3RyeVXUsEF_D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XlLW_es1-Z87",
      "metadata": {
        "id": "XlLW_es1-Z87"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# Lightning Module for Node2Vec model\n",
        "class Node2VecModel(pl.LightningModule):\n",
        "    def __init__(self, data, embedding_dim=32, walk_length=20, context_size=10, walks_per_node=10, num_negative_samples=1, p=1.0, q=1.0, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.model = Node2Vec(\n",
        "            data.edge_index,\n",
        "            embedding_dim=embedding_dim,\n",
        "            walk_length=walk_length,\n",
        "            context_size=context_size,\n",
        "            walks_per_node=walks_per_node,\n",
        "            num_negative_samples=num_negative_samples,\n",
        "            p=p,\n",
        "            q=q,\n",
        "            sparse=True\n",
        "        )\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self):\n",
        "        return self.model()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pos_rw, neg_rw = batch\n",
        "        loss = self.model.loss(pos_rw.to(self.device), neg_rw.to(self.device))\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SparseAdam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        z = self.model()\n",
        "        acc = self.model.test(\n",
        "            train_z=z[self.data.train_mask],\n",
        "            train_y=self.data.y[self.data.train_mask],\n",
        "            test_z=z[self.data.test_mask],\n",
        "            test_y=self.data.y[self.data.test_mask],\n",
        "            max_iter=150\n",
        "        )\n",
        "        self.log('test_acc', acc)\n",
        "        return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = 4\n",
        "graph_embed = []\n",
        "\n",
        "for el in [spatG,hourG,seqG,dayG]:\n",
        "\n",
        "  model = Node2VecModel(data=el)\n",
        "\n",
        "  loader = model.model.loader(batch_size=128, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=1,\n",
        "      callbacks=[\n",
        "          ModelCheckpoint(monitor='train_loss'),\n",
        "          EarlyStopping(monitor='train_loss', patience=10)\n",
        "      ]\n",
        "  )\n",
        "  trainer.fit(model, loader)\n",
        "  #trainer.test(model, loader)\n",
        "\n",
        "  # Plotting points\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  z = model().detach()\n",
        "  graph_embed.append(z)\n",
        "\n",
        "  #z = TSNE(n_components=2).fit_transform(z)\n",
        "  print(z.shape)\n"
      ],
      "metadata": {
        "id": "-pCdEuFKhgcr"
      },
      "id": "-pCdEuFKhgcr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QxAZBv8Y-udN",
      "metadata": {
        "id": "QxAZBv8Y-udN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#train_x,test_x,train_y,test_y = train_test_split(seq,torch.stack(next),test_size=0.2,stratify=torch.stack(next))\n",
        "train_x,test_x,train_y,test_y = train_test_split(seq,torch.stack(next),test_size=0.2)\n",
        "#train_x,val_x,train_y,val_y = train_test_split(train_x,train_y,test_size=0.2,stratify=train_y)\n",
        "train_x,val_x,train_y,val_y = train_test_split(train_x,train_y,test_size=0.2)\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(train_x,train_y)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=batchsize,shuffle=True)\n",
        "print(len(trainloader))\n",
        "\n",
        "valset = torch.utils.data.TensorDataset(val_x,val_y)\n",
        "valloader = torch.utils.data.DataLoader(valset,batch_size=batchsize,shuffle=True)\n",
        "print(len(valloader))\n",
        "\n",
        "testset = torch.utils.data.TensorDataset(test_x,test_y)\n",
        "testloader = torch.utils.data.DataLoader(testset,batch_size=batchsize,shuffle=True)\n",
        "print(len(testloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qSHVr4HM5ddS",
      "metadata": {
        "id": "qSHVr4HM5ddS"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "'''\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        #print(x)\n",
        "        #print(\"sono una nuova convoluzione\")\n",
        "        #print(edge_index)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "'''\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, location_dim, sequence_dim,graph_embed):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.graph_embed = graph_embed\n",
        "\n",
        "        # User Embedding Layer\n",
        "        self.user_embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=32)\n",
        "\n",
        "        #Last location Embedding Layer\n",
        "        self.lastLoc_embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=32)\n",
        "\n",
        "        #Time and day Input linear\n",
        "\n",
        "        self.timeday_embedding = nn.Linear(2,32)\n",
        "        '''\n",
        "        # Location Embedding Layers\n",
        "        self.gcn1 = GCN(spatG.num_features, 64)\n",
        "        self.gcn2 = GCN(seqG.num_features, 64)\n",
        "        self.gcn3 = GCN(hourG.num_features, 64)\n",
        "        self.gcn4 = GCN(dayG.num_features, 64)\n",
        "        '''\n",
        "\n",
        "        # Sequence Embedding Layer\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=sequence_dim, num_heads=1)\n",
        "\n",
        "        #Expand sequence representation\n",
        "\n",
        "        self.expand_sequence = nn.Linear(sequence_dim,32)\n",
        "\n",
        "        #Reduce embed dimensionality\n",
        "\n",
        "        #self.compress_embed = nn.Linear()\n",
        "\n",
        "        # Multihead Attention Layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=32, num_heads=2)\n",
        "\n",
        "        #print(len(spatG.x))\n",
        "\n",
        "        # Classifier Layers\n",
        "        self.linear1 = nn.Linear(int((4*32*(self.graph_embed[0].shape[0]+batchsize))/batchsize), 128)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.linear3 = nn.Linear(64, numvenues)\n",
        "\n",
        "    def forward(self, user, lastloc, lasthour, lastday, sequence):\n",
        "        #print(lastday)\n",
        "        #curr_batch = len(user)\n",
        "        #print(len(user))\n",
        "\n",
        "\n",
        "        user_embed = self.user_embedding(user.int())\n",
        "\n",
        "        lastloc_embed = self.lastLoc_embedding(lastloc.int())\n",
        "\n",
        "        timeday_embed = self.timeday_embedding(torch.stack([lasthour.float(), lastday.float()],dim=1))\n",
        "\n",
        "\n",
        "        graph_embed = torch.cat(self.graph_embed)\n",
        "        #print(graph_embed.shape)\n",
        "        #nodes = spatG.x[lastloc]\n",
        "\n",
        "        #print(nodes)\n",
        "\n",
        "        #k = 1\n",
        "        # Location Embeddings\n",
        "        #nodes,edge_index1,mapping,_ =torch_geometric.utils.k_hop_subgraph(lastloc,k,spatG.edge_index)\n",
        "        #print(len(nodes))\n",
        "\n",
        "        #nodes = spatG.x[nodes]\n",
        "\n",
        "        #print(nodes)\n",
        "\n",
        "        #print(isinstance(spatG.edge_index, torch.Tensor))\n",
        "        #print(spatG.edge_index)\n",
        "\n",
        "        '''\n",
        "\n",
        "        location_embed = self.gcn1(x=spatG.x, edge_index=spatG.edge_index)\n",
        "        seq_embed = self.gcn2(x=seqG.x, edge_index=seqG.edge_index)\n",
        "        hour_embed = self.gcn3(x=hourG.x,edge_index=hourG.edge_index)\n",
        "        day_embed =  self.gcn4(x=dayG.x,edge_index=dayG.edge_index)\n",
        "        '''\n",
        "        #print(sequence)\n",
        "\n",
        "        # Sequence Embedding\n",
        "        sequence_embed, _ = self.self_attn(sequence.float(), sequence.float(), sequence.float())\n",
        "\n",
        "        sequence_embed = self.expand_sequence(sequence_embed)\n",
        "\n",
        "        '''\n",
        "        print(user_embed.shape)\n",
        "        #print(location_embed.shape)\n",
        "        #print(seq_embed.shape)\n",
        "        #print(hour_embed.shape)\n",
        "        #print(day_embed.shape)\n",
        "        print(sequence_embed.shape)\n",
        "        print(lastloc_embed.shape)\n",
        "        print(timeday_embed.shape)\n",
        "        '''\n",
        "        #user_embed = user_embed*sequence_embed\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        concat_embed = torch.cat([user_embed,lastloc_embed,graph_embed,timeday_embed,sequence_embed], dim=0)\n",
        "\n",
        "        #concat_embed = concat_embed.view(batchsize,-1)\n",
        "\n",
        "\n",
        "        #print(concat_embed.shape)\n",
        "        #print(len(spatG.x))\n",
        "\n",
        "        # Multihead Attention\n",
        "        attn_output, _ = self.multihead_attn(concat_embed, concat_embed, concat_embed)\n",
        "\n",
        "        #print(attn_output.shape)\n",
        "\n",
        "\n",
        "        # Classifier\n",
        "        out = F.relu(self.linear1(attn_output.view(batchsize,-1)))\n",
        "        out = F.relu(self.linear2(out))\n",
        "        out = self.linear3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define Lightning model using PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim, location_dim, sequence_dim,graph_embed):\n",
        "        super(LightningModel, self).__init__()\n",
        "        self.model = Model(input_dim, location_dim, sequence_dim,graph_embed)\n",
        "\n",
        "    def forward(self, user, timestamp, location, sequence):\n",
        "        return self.model(user, timestamp, location, sequence)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        x, y = batch\n",
        "        #print(x.shape,y.shape)\n",
        "        pad_dim = batchsize-x.shape[0]\n",
        "\n",
        "        if pad_dim != 0:\n",
        "\n",
        "         x = torch.cat([x,torch.zeros(pad_dim,x.shape[1])])\n",
        "         y =torch.cat([y,torch.zeros(pad_dim,y.shape[1])])\n",
        "\n",
        "        #print(x)\n",
        "        y_pred = self.model(x[:,0],x[:,1],x[:,2],x[:,3],x[:,4:])\n",
        "        loss = F.cross_entropy(torch.sigmoid(y_pred), y)\n",
        "        print(loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        return optimizer\n",
        "\n",
        "# Example of training the model\n",
        "input_dim = 10000\n",
        "location_dim = 32\n",
        "sequence_dim = seq_dim\n",
        "#print(seq_dim)\n",
        "model = LightningModel(input_dim, location_dim, sequence_dim,graph_embed)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10)\n",
        "trainer.fit(model, trainloader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}